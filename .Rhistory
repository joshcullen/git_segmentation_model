x
x$vol<0.5
setwd('U:\\teste')
dat=read.csv('esteban.csv')
?read.csv
head(dat[,1:3])
?write.csv
?mean
setwd('U:\\teste')
dat=read.csv('esteban.csv',as.is=T)
setwd('U:\\teste')
dat=read.csv('esteban.csv')
str(dat)
dat$software
unique(dat$software)
setwd('U:\\teste')
dat=read.csv('esteban.csv',as.is=T)
str(dat)
x=data.frame(tree.id=1:10,
vol=runif(10))
hist(x$vol)
setwd('U:\\teste')
setwd('U:\\teste')
png('volume histogram.png',width=700,heigth=700)
hist(x$vol)
dev.off()
png('volume histogram.png',width=700,height=700)
hist(x$vol)
dev.off()
rivers
summary(rivers)
summary('rivers')
summary(cars)
summary('cars')
data()
rm(list=ls(all=TRUE))
set.seed(1)
setwd('U:\\poli\\segmentation')
nobs=1000
nseg=10
tmp=runif(nseg)
prob=tmp/sum(tmp); prob
partition=rmultinom(1,size=nobs,prob=prob)
mu=runif(nseg,min=5,max=50)
seg.index=rep(1:nseg,times=partition)
obs=data.frame(obs=rnorm(nobs,mean=mu[seg.index],sd=1),
time1=1:nobs)
plot(obs)
write.csv(obs,'fake data.csv',row.names=F)
rm(list=ls(all=TRUE))
set.seed(1)
setwd('U:\\poli\\segmentation')
dat=read.csv('fake data.csv',as.is=T)
rm(list=ls(all=TRUE))
set.seed(1)
setwd('U:\\poli\\segmentation')
dat=read.csv('fake data.csv',as.is=T)
#priors
tau2=100
mu0=0
breakpt
mean(dat$time1)
rm(list=ls(all=TRUE))
set.seed(1)
setwd('U:\\poli\\segmentation')
dat=read.csv('fake data.csv',as.is=T)
#priors
tau2=100
mu0=0
#starting values
breakpt=mean(dat$time1)
breakpt.old=breakpt
p=length(breakpt)
rand1=runif(1)
p
p0=1
breakpt.new=sort(c(breakpt.old,runif(1,0,max.time)))
p0=1/3 #death prob 2 -> 1 is (1/3) and birth prob 1
max.time=max(dat$time1)
breakpt.new=sort(c(breakpt.old,runif(1,0,max.time)))
p0=1/3 #death prob 2 -> 1 is (1/3) and birth prob 1 -> 2 is 1.
breakpt.new
runif(1,0,max.time)
runif(1,0,max.time)
runif(1,0,max.time)
runif(1,min=0,max=max.time)
max.time
breakpt.new
stats.old=get.summary.stats(breakpt=breakpt.old,dat=dat)
stats.new=get.summary.stats(breakpt=breakpt.new,dat=dat)
setwd('U:\\poli\\segmentation')
source('gibbs functions.R')
rm(list=ls(all=TRUE))
set.seed(1)
nobs=1000
nseg=10
tmp=runif(nseg)
prob=tmp/sum(tmp); prob
partition=rmultinom(1,size=nobs,prob=prob)
mu=runif(nseg,min=5,max=50)
seg.index=rep(1:nseg,times=partition)
obs=data.frame(obs=rnorm(nobs,mean=mu[seg.index],sd=1),
time1=1:nobs)
plot(obs)
setwd('U:\\GIT_models\\git_segmentation_model')
write.csv(obs,'fake data.csv',row.names=F)
rm(list=ls(all=TRUE))
set.seed(1)
setwd('U:\\GIT_models\\git_segmentation_model')
source('gibbs functions.R')
dat=read.csv('fake data.csv',as.is=T)
#priors
tau2=100
mu0=0
#useful stuff
max.time=max(dat$time1)
#starting values
breakpt=mean(dat$time1)
ngibbs=1000
breakpt.old=breakpt
p=length(breakpt)
rand1=runif(1)
p0=1
new.brk=runif(1,min=0,max=max.time)
if (p == 1) {
#birth
if (rand1 < 1/2){
breakpt.new=sort(c(breakpt.old,new.brk))
p0=2/3 #death prob 2 -> 1 is (1/3) and birth prob 1 -> 2 is 1/2.
}
#swap
if (rand1 > 1/2) breakpt.new=new.brk
}
if (p > 1) {
#birth
if (rand1 < 1/3) {
breakpt.new=sort(c(breakpt.old,new.brk))
}
#death
if (rand1 > 1/3 & rand1 < 2/3) {
ind=sample(1:length(breakpt.old),size=1)
breakpt.new=breakpt.old[-ind]
if (p==2) p0=3/2 #birth prob from 1 -> 2 is 1/2 and death prob from 2 -> 1 is 1/3
}
#swap
if (rand1 > 2/3) {
ind=sample(1:length(breakpt.old),size=1)
breakpt.new=sort(c(breakpt.old[-ind],new.brk))
}
}
breakpt.old
breakpt.new
stats.old=get.summary.stats(breakpt=breakpt.old,dat=dat)
stats.new=get.summary.stats(breakpt=breakpt.new,dat=dat)
stats.old
stats.new
pold=log.marg.likel(tau2=tau2,mu0=mu0,summary.stats=stats.old)
pold
summary.stats=stats.old
inv.sig2=summary.stats[,'n']+(1/tau2)
sig2=1/inv.sig2
#get mu1
num1=summary.stats[,'sum.x']+(1/tau2)*mu0
den1=summary.stats[,'n']+(1/tau2)
mu1=num1/den1
#get pieces of equation
p1=summary.stats[,'n']*log(2*pi)
p2=log(sig2/tau2)
p3=(mu1^2)/sig2
p4=summary.stats[,'sum.x2']+((mu0^2)/tau2)
p1
p2
p3
p4
log.marg.likel=function(tau2,mu0,summary.stats){
#get sig2
inv.sig2=summary.stats[,'n']+(1/tau2)
sig2=1/inv.sig2
#get mu1
num1=summary.stats[,'sum.x']+(1/tau2)*mu0
den1=summary.stats[,'n']+(1/tau2)
mu1=num1/den1
#get pieces of equation
p1=summary.stats[,'n']*log(2*pi)
p2=log(sig2/tau2)
p3=(mu1^2)/sig2
p4=summary.stats[,'sum.x2']+((mu0^2)/tau2)
sum((1/2)*(-p1+p2+p3-p4))
}
stats.old=get.summary.stats(breakpt=breakpt.old,dat=dat)
stats.new=get.summary.stats(breakpt=breakpt.new,dat=dat)
pold=log.marg.likel(tau2=tau2,mu0=mu0,summary.stats=stats.old)
pnew=log.marg.likel(tau2=tau2,mu0=mu0,summary.stats=stats.new)+log(p0)
pold
pnew
prob=exp(pnew-pold)
prob
rm(list=ls(all=TRUE))
set.seed(1)
setwd('U:\\GIT_models\\git_segmentation_model')
source('gibbs functions.R')
dat=read.csv('fake data.csv',as.is=T)
#priors
tau2=100
mu0=0
#useful stuff
max.time=max(dat$time1)
#starting values
breakpt=mean(dat$time1)
ngibbs=1000
for (i in 1:ngibbs){
print(i)
breakpt=samp.move(breakpt=breakpt,max.time=max.time,dat=dat,tau2=tau2,mu0=mu0)
}
breakpt
abline(v=breakpt)
length(breakpt)
rm(list=ls(all=TRUE))
set.seed(1)
nobs=1000
nseg=10
tmp=runif(nseg)
prob=tmp/sum(tmp); prob
partition=rmultinom(1,size=nobs,prob=prob)
mu=runif(nseg,min=5,max=50)
seg.index=rep(1:nseg,times=partition)
obs=data.frame(obs=rnorm(nobs,mean=mu[seg.index],sd=1),
time1=1:nobs)
plot(obs~time1,data=obs)
setwd('U:\\GIT_models\\git_segmentation_model')
write.csv(obs,'fake data.csv',row.names=F)
rm(list=ls(all=TRUE))
set.seed(1)
setwd('U:\\GIT_models\\git_segmentation_model')
source('gibbs functions.R')
dat=read.csv('fake data.csv',as.is=T)
#priors
tau2=100
mu0=0
#useful stuff
max.time=max(dat$time1)
#starting values
breakpt=mean(dat$time1)
ngibbs=1000
for (i in 1:ngibbs){
print(i)
breakpt=samp.move(breakpt=breakpt,max.time=max.time,dat=dat,tau2=tau2,mu0=mu0)
}
abline(h=breakpt,col='grey')
abline(v=breakpt,col='grey')
rm(list=ls(all=TRUE))
set.seed(1)
nobs=1000
nseg=10
tmp=runif(nseg)
prob=tmp/sum(tmp); prob
partition=rmultinom(1,size=nobs,prob=prob)
mu=runif(nseg,min=5,max=50)
seg.index=rep(1:nseg,times=partition)
obs=data.frame(obs=rnorm(nobs,mean=mu[seg.index],sd=1),
time1=1:nobs)
plot(obs~time1,data=obs)
setwd('U:\\GIT_models\\git_segmentation_model')
write.csv(obs,'fake data.csv',row.names=F)
rm(list=ls(all=TRUE))
set.seed(1)
setwd('U:\\GIT_models\\git_segmentation_model')
source('gibbs functions.R')
dat=read.csv('fake data.csv',as.is=T)
#priors
tau2=100
mu0=0
#useful stuff
max.time=max(dat$time1)
#starting values
breakpt=mean(dat$time1)
ngibbs=10000
for (i in 1:ngibbs){
print(i)
breakpt=samp.move(breakpt=breakpt,max.time=max.time,dat=dat,tau2=tau2,mu0=mu0)
}
length(breakpt)
abline(v=breakpt,col='grey')
set.seed(1)
library(dplyr)
library(lubridate)
library(ggplot2)
library(coda)
library(raster)
source('gibbs functions2.R')
source('helper functions.R')
dat<- read.csv("Snail Kite Gridded Data.csv", header = T, sep = ",")
#Update class and/or values of vars
dat$ESTtime<- as_datetime(dat$ESTtime)
grid_5<- raster(extent(min(dat$utmlong), max(dat$utmlong),
min(dat$utmlat), max(dat$utmlat)) + 10000)
res(grid_5)<- 5000
proj4string(grid_5)<- CRS("+init=epsg:32617")
grid_5[]<- 0
#Create occupancy matrices
obs<- matrix(0, nrow(dat), length(grid_5))
for (i in 1:nrow(dat)){
obs[i,dat$grid.cell[i]]=1
}
colnames(obs)=paste0('grid.cell_',1:length(grid_5))
obs<- obs[,apply(obs,2,sum) != 0]
obs<- cbind(dat$id, obs)
colnames(obs)[1]<- "id"
obs1=data.frame(obs) %>% filter(id == 1) %>% dplyr::select(-id)
obs12=data.frame(obs) %>% filter(id == 12) %>% dplyr::select(-id)
obs19=data.frame(obs) %>% filter(id == 19) %>% dplyr::select(-id)
obs27=data.frame(obs) %>% filter(id == 27) %>% dplyr::select(-id)
obs1$time1=1:nrow(obs1)
obs12$time1=1:nrow(obs12)
obs19$time1=1:nrow(obs19)
obs27$time1=1:nrow(obs27)
setwd("~/Documents/Snail Kite Project/Data")
obs<- read.csv("Occupancy Matrix for all Obs and Locs.csv", header = T, sep = ",")
obs1.breakpts<- read.csv("ID1 Breakpoints (5 km).csv", header = T, sep = ",")
obs1.breakpts=obs1.breakpts[,1]
obs12.breakpts<- read.csv("ID12 Breakpoints (5 km).csv", header = T, sep = ",")
obs12.breakpts=obs12.breakpts[,1]
obs19.breakpts<- read.csv("ID19 Breakpoints (5 km).csv", header = T, sep = ",")
obs19.breakpts=obs19.breakpts[,1]
obs27.breakpts<- read.csv("ID27 Breakpoints (5 km).csv", header = T, sep = ",")
obs27.breakpts=obs27.breakpts[,1]
obs1=data.frame(obs) %>% filter(id == 1) %>% dplyr::select(-id)
obs12=data.frame(obs) %>% filter(id == 12) %>% dplyr::select(-id)
obs19=data.frame(obs) %>% filter(id == 19) %>% dplyr::select(-id)
obs27=data.frame(obs) %>% filter(id == 27) %>% dplyr::select(-id)
obs1$time1=1:nrow(obs1)
obs12$time1=1:nrow(obs12)
obs19$time1=1:nrow(obs19)
obs27$time1=1:nrow(obs27)
nloc=ncol(obs)-1 #remove time1
obs1.seg=get.summary.stats(obs1.breakpts,obs1,nloc)
obs12.seg=get.summary.stats(obs12.breakpts,obs12,nloc)
obs19.seg=get.summary.stats(obs19.breakpts,obs19,nloc)
obs27.seg=get.summary.stats(obs27.breakpts,obs27,nloc)
dat1=dat %>% filter(id==1)
dat1<- assign.time.seg(obs1.seg, obs1.breakpts, dat1)
dat12=dat %>% filter(id==12)
dat12<- assign.time.seg(obs12.seg, obs12.breakpts, dat12)
dat19=dat %>% filter(id==19)
dat19<- assign.time.seg(obs19.seg, obs19.breakpts, dat19)
dat27=dat %>% filter(id==27)
dat27<- assign.time.seg(obs27.seg, obs27.breakpts, dat27)
dat.list<- list(`1`=dat1, `12`=dat12, `19`=dat19, `27`=dat27)
kmeans.list<- vector("list", length(dat.list))
for (i in 1:length(dat.list)) {
dat.cells<- grid.summary.table(dat = dat.list[[i]], crs = CRS('+init=epsg:32617'))  #not working for IDs 12-27; check this function
dat.cells<- kmeans.cluster(dat.cells)
dat<- left_join(dat.list[[i]], dat.cells[,-c(2:3)], by = "grid.cell")
obs.kmeans<- get.summary.stats_kmeans(dat)
kmeans.list[[i]]<- obs.kmeans
}
i=1
dat.cells<- grid.summary.table(dat = dat.list[[i]], crs = CRS('+init=epsg:32617'))  #not working for IDs 12-27; check this function
dat.cells<- kmeans.cluster(dat.cells)
dat<- left_join(dat.list[[i]], dat.cells[,-c(2:3)], by = "grid.cell")
obs.kmeans<- get.summary.stats_kmeans(dat)
View(obs.kmeans)
library('Rcpp')
library('MCMCpack')
setwd("~/Documents/Snail Kite Project/Data/R Scripts/cluster_tsegments_loc")
sourceCpp('aux1.cpp')
source('gibbs functions.R') #for clustering
source('helper functions.R') #for prepping data
dat<- obs.kmeans
n=rowSums(dat)
nobs=nrow(dat)
nloc=ncol(dat)
lo=0.000000000000001
#priors
psi=0.01
gamma1=0.1
nclustmax=5
z=sample(1:nclustmax,size=nobs,replace=T)
theta=matrix(1/nloc,nclustmax,nloc)
phi=rep(1/nclustmax,nclustmax)
#store results
ngibbs=1000
store.phi=matrix(NA,ngibbs,nclustmax)
store.z=matrix(NA,ngibbs,nobs)
store.theta=matrix(NA,ngibbs,nclustmax*nloc)
store.loglikel=matrix(NA,ngibbs,1)
#gibbs sampler
nburn=ngibbs/2
for (i in 1:ngibbs){
print(i)
#occasionally re-order this
if (i<nburn & i%%50==0){
ind=order(phi,decreasing=T)
theta=theta[ind,]
phi=phi[ind]
znew=z
for (j in 1:nclustmax){
znew[z==ind[j]]=j
}
z=znew
}
#draw samples from FCD's
z=sample.z(dat=dat,theta=theta,phi=phi,
nobs=nobs,nclustmax=nclustmax,nloc=nloc,z=z,n=n)
# z=z.true
v=sample.v(z=z,nclustmax=nclustmax,gamma1=gamma1)
phi=GetPhi(vec=c(v,1),nclustmax=nclustmax)
theta=sample.theta(dat=dat,nclustmax=nclustmax,nloc=nloc,z=z,psi=psi)
#to avoid numerical issues
theta[theta<lo]=lo
# theta=theta.true
#get logl
tmp=sum(dat*log(theta)[z,])+sum(dbeta(v,1,gamma1,log=T))+sum((psi-1)*log(theta))
#store results
store.loglikel[i]=tmp
store.theta[i,]=theta
store.phi[i,]=phi
store.z[i,]=z
}
plot(store.loglikel,type='l')
max(store.loglikel)
which(max(store.loglikel))
store.log.likel[which(max(store.loglikel))]
store.loglikel[which(max(store.loglikel))]
store.loglikel[which(max(store.loglikel)),]
which(store.loglikel==max(store.loglikel))
MAP1<- which(store.loglikel==max(store.loglikel))  #895 iteration is MAP
tbsp1.clust<- store.z[MAP1,]
time.seg<- 1:49
tbsp.clust<- cbind(tbsp.clust1,time.seg) %>% data.frame()
tbsp1.clust<- store.z[MAP1,]
tbsp1.clust<- cbind(tbsp1.clust,time.seg) %>% data.frame()
tbsp1.clust$tbsp1.clust<- as.factor(tbsp1.clust$tbsp1.clust)
levels(tbsp1.clust$tbsp1.clust)<- 1:length(levels(tbsp1.clust$tbsp1.clust))
dat1$time.seg<- as.factor(dat1$time.seg)
tbsp1.clust$time.seg<- as.factor(tbsp1.clust$time.seg)
dat1<- left_join(dat1, tbsp1.clust, by="time.seg")
View(dat1)
tbsp.clust<- store.z[MAP1,]
time.seg<- 1:49
tbsp.clust<- cbind(tbsp.clust,time.seg) %>% data.frame()
tbsp.clust$tbsp.clust<- as.factor(tbsp.clust$tbsp.clust)
levels(tbsp.clust$tbsp.clust)<- 1:length(levels(tbsp.clust$tbsp.clust))
dat1$time.seg<- as.factor(dat1$time.seg)
tbsp.clust$time.seg<- as.factor(tbsp.clust$time.seg)
dat1<- left_join(dat1, tbsp.clust, by="time.seg")
View(dat)
colnames(dat)=1:ncol(dat)
View(dat)
obs1.breakpts<- data.frame(breaks=obs1.breakpts)
nrow(dat)
ncol(dat)
obs1.long<- dat %>% data.frame() %>% gather(key, value) %>% mutate(time=rep(1:nobs, times=nloc))
library(tidyr)
obs1.long<- dat %>% data.frame() %>% gather(key, value) %>% mutate(time=rep(1:nobs, times=nloc))
obs1.long$key<- as.factor(obs1.long$key)
levels(obs1.long$key)<- 1:nloc
obs1.long$key<- as.numeric(obs1.long$key)
tbsp.clust[,1]<- tbsp.clust[,1] %>% as.numeric()
tbsp.clust[,2]<- tbsp.clust[,2] %>% as.numeric()
rect.lims<- rle(tbsp.clust$tbsp.clust)
rect.lims$lengths<- cumsum(rect.lims$lengths)+0.5
rect.lims$lengths<- c(0.5, rect.lims$lengths)
rect.lims.new<- matrix(0, length(rect.lims$values), 3)
for (i  in 2:length(rect.lims$lengths)) {
rect.lims.new[i-1,]<- c(rect.lims$lengths[i-1], rect.lims$lengths[i], rect.lims$values[i-1])
}
colnames(rect.lims.new)<- c("xmin","xmax","tbsp.clust")
rect.lims.new<- data.frame(rect.lims.new)
ggplot() +
geom_tile(data=obs1.long, aes(x=time, y=key, fill=log10(value+1))) +
scale_fill_viridis_c("log10(N+1)") +
scale_y_continuous(breaks = 1:6, expand = c(0,0)) +
scale_x_continuous(expand = c(0,0)) +
new_scale_fill() +
geom_vline(data = rect.lims.new, aes(xintercept = xmin), color = "white", size = 0.35) +
geom_rect(data=rect.lims.new, aes(xmin = xmin, xmax = xmax, ymin = 6.5,
ymax = 6.75, fill = tbsp.clust), color = NA, size = 1.5) +
scale_fill_gradientn("Time Cluster", colours = ocean.amp(6)) +
labs(x = "Time Segment", y = "Spatial Cluster") +
theme_bw() +
theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))
library(ggnewscale) #for multiple fill scales in ggplot2
ggplot() +
geom_tile(data=obs1.long, aes(x=time, y=key, fill=log10(value+1))) +
scale_fill_viridis_c("log10(N+1)") +
scale_y_continuous(breaks = 1:6, expand = c(0,0)) +
scale_x_continuous(expand = c(0,0)) +
new_scale_fill() +
geom_vline(data = rect.lims.new, aes(xintercept = xmin), color = "white", size = 0.35) +
geom_rect(data=rect.lims.new, aes(xmin = xmin, xmax = xmax, ymin = 6.5,
ymax = 6.75, fill = tbsp.clust), color = NA, size = 1.5) +
scale_fill_gradientn("Time Cluster", colours = ocean.amp(6)) +
labs(x = "Time Segment", y = "Spatial Cluster") +
theme_bw() +
theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))
library(pals) # for more color palettes
ggplot() +
geom_tile(data=obs1.long, aes(x=time, y=key, fill=log10(value+1))) +
scale_fill_viridis_c("log10(N+1)") +
scale_y_continuous(breaks = 1:6, expand = c(0,0)) +
scale_x_continuous(expand = c(0,0)) +
new_scale_fill() +
geom_vline(data = rect.lims.new, aes(xintercept = xmin), color = "white", size = 0.35) +
geom_rect(data=rect.lims.new, aes(xmin = xmin, xmax = xmax, ymin = 6.5,
ymax = 6.75, fill = tbsp.clust), color = NA, size = 1.5) +
scale_fill_gradientn("Time Cluster", colours = ocean.amp(6)) +
labs(x = "Time Segment", y = "Spatial Cluster") +
theme_bw() +
theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))
ggplot() +
geom_sf(data = fl)
